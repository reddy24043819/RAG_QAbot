{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
        "\n",
        "##**Summary :**\n",
        "\n",
        "This Colab notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot designed for business use. The QA bot is capable of handling queries related to a provided document or dataset, retrieving relevant information, and generating coherent responses using a generative model (Cohere API).\n",
        "\n",
        "The key components of the model include:\n",
        "\n",
        "* Document Processing: Extracting text from the document and splitting it into manageable chunks.\n",
        "* Vector Database: Using FAISS for efficient storage and retrieval of document embeddings.\n",
        "* Query Handling: For each user query, the model retrieves relevant document chunks using FAISS and generates a contextually accurate answer with Cohere API.\n",
        "\n",
        "**This notebook covers the entire pipeline from data loading, document embedding, query processing, retrieval, and answer generation. It includes several example queries to showcase how the system performs in retrieving relevant document segments and generating accurate answers.**\n",
        "\n",
        "Deliverables:\n",
        "\n",
        "An end-to-end demonstration of the RAG model pipeline.\n",
        "Explanation of the architecture, retrieval approach, and generative response mechanism.\n",
        "Examples showing the effectiveness of the model in answering queries based on the document content.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ouJXPLbFbDFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-INu2voamUP",
        "outputId": "4978f5a4-faa8-4f6b-f4fe-67c74fd5eeb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi this is a colab notebook for QA bot \n"
          ]
        }
      ],
      "source": [
        "print(\"Hi this is a colab notebook for QA bot \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup Environment"
      ],
      "metadata": {
        "id": "WC06ugOsaxHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use Python for this project in a Google Colab environment.\n",
        "\n",
        "Packages Required:\n",
        "* Transformers: To use pre-trained generative models.\n",
        "* Cohere: Cohere API for text generation.\n",
        "* Faiss (alternative to Pinecone if not available): For vector similarity search.\n",
        "* Streamlit or Gradio: For an interactive UI in Part 2.\n",
        "* PDFPlumber: To handle PDF document processing.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "WhZNwLmPa8GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0ESpPyea7hg",
        "outputId": "86310240-8938-42e1-c9f5-67da64837f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Load and Process the Dataset\n",
        "* First, we need to load and pre-process the document or dataset for which we are building the QA bot.\n",
        "* Assuming it's a text-based document:\n",
        " Extract content from the document (in the PDF format).\n",
        "* Tokenize or segment the document into chunks suitable for embedding generation."
      ],
      "metadata": {
        "id": "is1rEdaBb3MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting text from the pdf"
      ],
      "metadata": {
        "id": "IbtIofZUs0A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEzsPpRMch7o",
        "outputId": "d56bcc54-b8d1-4dd9-d48a-ce7fffb2279f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "# Function to load and extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "document_path = \"/content/Gen AI Engineer _ Machine Learning Engineer Assignment.pdf\"\n",
        "document_text = extract_text_from_pdf(document_path)\n",
        "print(document_text) #uncomment to see the document text extracted from pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubz0-Yu3bx4H",
        "outputId": "0e8aa9fe-921e-4dc7-ce14-4b9bb2111c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen AI Engineer / Machine Learning Engineer Assignment\n",
            "Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
            "Problem Statement:\n",
            "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
            "bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
            "Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
            "relevant information from a dataset and generate coherent answers.\n",
            "Task Requirements:\n",
            "1. Implement a RAG-based model that can handle questions related to a provided\n",
            "document or dataset.\n",
            "2. Use a vector database (such as Pinecone) to store and retrieve document\n",
            "embeddings efficiently.\n",
            "3. Test the model with several queries and show how well it retrieves and generates\n",
            "accurate answers from the document.\n",
            "Deliverables:\n",
            "● A Colab notebook demonstrating the entire pipeline, from data loading to question\n",
            "answering.\n",
            "● Documentation explaining the model architecture, approach to retrieval, and how\n",
            "generative responses are created.\n",
            "● Provide several example queries and the corresponding outputs.Part 2: Interactive QA Bot Interface\n",
            "Problem Statement:\n",
            "Develop an interactive interface for the QA bot from Part 1, allowing users to input queries\n",
            "and retrieve answers in real time. The interface should enable users to upload documents\n",
            "and ask questions based on the content of the uploaded document.\n",
            "Task Requirements:\n",
            "1. Build a simple frontend interface using Streamlit or Gradio, allowing users to\n",
            "upload PDF documents and ask questions.\n",
            "2. Integrate the backend from Part 1 to process the PDF, store document embeddings,\n",
            "and provide real-time answers to user queries.\n",
            "3. Ensure that the system can handle multiple queries efficiently and provide accurate,\n",
            "contextually relevant responses.\n",
            "4. Allow users to see the retrieved document segments alongside the generated\n",
            "answer.\n",
            "Deliverables:\n",
            "● A deployed QA bot with a frontend interface where users can upload documents and\n",
            "interact with the bot.\n",
            "● Documentation on how the user can upload files, ask questions, and view the bot's\n",
            "responses.\n",
            "● Example interactions demonstrating the bot's capabilities.\n",
            "Guidelines:\n",
            "● Use Docker to containerize the application for easy deployment.\n",
            "● Ensure the system can handle large documents and multiple queries without\n",
            "significant performance drops.\n",
            "● Share the code, deployment instructions, and the final working model through\n",
            "GitHub.\n",
            "General Guidelines:\n",
            "1. Ensure modular and scalable code following best practices for both frontend and\n",
            "backend development.\n",
            "2. Document your approach thoroughly, explaining your decisions, challenges faced,\n",
            "and solutions.\n",
            "3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\n",
            "instructions.\n",
            "4. Submissions should include:\n",
            "○ Source code for both the notebook and the interface.\n",
            "○ A fully functional Colab notebook.\n",
            "○ Documentation on the pipeline and deployment instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Create Embeddings Using a Pre-trained Model :\n",
        "* To handle document retrieval, we need to convert the document into embeddings using a model like SentenceTransformers.\n",
        "* These embeddings capture the semantic meaning of the text and are used for similarity search."
      ],
      "metadata": {
        "id": "I-go-pblcw4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JusJGCvM5Kh",
        "outputId": "1967ccc1-8634-4823-9f0a-96be1ead5b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the SentenceTransformer Model :**\n",
        "A pre-trained SentenceTransformer model (all-MiniLM-L6-v2) is loaded. This model has been trained on a massive dataset and is capable of generating high-quality sentence embeddings.\n",
        "* **Split Document Text into Chunks:**The document text is divided into smaller chunks of 300 characters with a 512 character stride. This ensures that the model can handle longer documents effectively.\n",
        "Generate Embeddings:\n",
        "* **The SentenceTransformer model** is used to encode each document chunk into a dense vector representation (embedding). These embeddings capture the semantic meaning of the text, allowing us to compare the similarity between different chunks and the user's query."
      ],
      "metadata": {
        "id": "6NwnvBJu21c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Split document into chunks for embedding\n",
        "document_chunks = [document_text[i:i + 300] for i in range(0, len(document_text), 512)]\n",
        "document_embeddings = model.encode(document_chunks)\n"
      ],
      "metadata": {
        "id": "BA5dI6nocwMN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Setup FAISS for Vector Storage\n",
        "\n",
        "\n",
        "\n",
        "**FAISS (Facebook AI Similarity Search) is an efficient library used for similarity search and clustering of dense vectors. In this step, we utilize FAISS to store and index the document embeddings generated in the previous step.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tf9VC9KUdXFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import the FAISS library (faiss) for similarity search.\n",
        "\n",
        "* Define Embedding Dimension:\n",
        "Set the embedding_dim variable to 384, which is the fixed dimension of the embeddings generated by the SentenceTransformer model.\n",
        "* Build FAISS Index:\n",
        "Create a FAISS index using IndexFlatL2. This index uses the L2 distance metric to measure similarity between vectors.\n",
        "* Convert Embeddings to NumPy Array:\n",
        "Convert the document embeddings into a NumPy array and ensure they are in float32 format, as required by FAISS.\n",
        "* Add Embeddings to Index:\n",
        "Add the NumPy array of document embeddings to the FAISS index. This allows FAISS to efficiently search for similar vectors.\n",
        "\n",
        "**By indexing the embeddings, FAISS enables fast retrieval of relevant document chunks based on the similarity between the query embedding and the stored document embeddings.**"
      ],
      "metadata": {
        "id": "B9eqxgtC4k9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "embedding_dim = 384  # SentenceTransformer embedding dimension\n",
        "# Build FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance metric\n",
        "faiss_embeddings = np.array(document_embeddings).astype(np.float32)\n",
        "index.add(faiss_embeddings)"
      ],
      "metadata": {
        "id": "bS92MDWad3ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Query Processing and Retrieval\n",
        "For the retrieval step, when a question is asked, we:\n",
        "\n",
        "* Encode the query.\n",
        "* Retrieve the top-k relevant document chunks based on the cosine similarity of the query and the document embeddings."
      ],
      "metadata": {
        "id": "3Gjfb0nQe7b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Relevant Text to the Query (question) from Document Text\n",
        "By searching for the nearest neighbors in the FAISS index, the code identifies the most relevant document chunks based on semantic similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "NMyhTUhdDLlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update : *We Increase the nearest neighbors to top_k = 5* to get better context for the model in real time"
      ],
      "metadata": {
        "id": "ZT2rmPBBVzk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Encode the Query:**\n",
        "The user's query is first encoded into an embedding using the same SentenceTransformer model used to encode the document chunks. This ensures that the query and document chunks are represented in the same vector space.\n",
        "* **Retrieve Relevant Chunks:**\n",
        "The query embedding is used to search for the most similar document chunks within the FAISS index. The retrieve_relevant_chunks_faiss function performs this search, returning the top_k most relevant chunks and their corresponding distances to the query.\n",
        "* **Rank and Return Results:**\n",
        "The retrieved chunks are ranked based on their similarity to the query embedding (distance). The closer the distance, the more relevant the chunk is to the query."
      ],
      "metadata": {
        "id": "mUx-UlXMGZbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve relevant chunks using FAISS\n",
        "def retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_texts, top_k=5):\n",
        "    # Step 1: Encode the query into an embedding\n",
        "    query_embedding = model.encode([query])[0].astype(np.float32)  # Convert to float32 for FAISS compatibility\n",
        "\n",
        "    # Step 2: Reshape the query embedding for FAISS (it should be 2D)\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    # Step 3: Perform the search using the FAISS index\n",
        "    distances, indices = index.search(query_embedding, 2)  # Search for 2 nearest neighbors\n",
        "\n",
        "    # Step 4: Retrieve the corresponding text chunks based on indices\n",
        "    relevant_chunks = [document_texts[i] for i in indices[0]]  # Use document_texts instead of embeddings\n",
        "\n",
        "    return relevant_chunks, distances[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "Kly4Y2-HmGWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query 1\n",
        "Initially Let's search for 2 nearest neighbors of the query \"What is the problem statement?\" *Direct relevance in document*"
      ],
      "metadata": {
        "id": "zRv8OkapDsfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"What is the Problem Statement?\"\n",
        "\n",
        "# Use document_chunks as document_texts, since these are the actual text chunks\n",
        "relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "\n",
        "# Print the relevant text chunks and their distances\n",
        "for i, chunk in enumerate(relevant_chunks):\n",
        "    print(f\"Chunk {i + 1}: {chunk} (Distance: {distances[i]})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3EQLnx3G4Ay",
        "outputId": "3a37f10b-463d-40aa-ad24-c4cba5789ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:  Provide several example queries and the corresponding outputs.Part 2: Interactive QA Bot Interface\n",
            "Problem Statement:\n",
            "Develop an interactive interface for the QA bot from Part 1, allowing users to input queries\n",
            "and retrieve answers in real time. The interface should enable users to upload documents (Distance: 1.480650544166565)\n",
            "\n",
            "Chunk 2: Gen AI Engineer / Machine Learning Engineer Assignment\n",
            "Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
            "Problem Statement:\n",
            "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
            "bot for a business. Use a vector database like Pinecone DB and a generative model (Distance: 1.5173135995864868)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query 2\n",
        "How is GitHub useful?\n",
        "*Indirect relevance in document*"
      ],
      "metadata": {
        "id": "FzLZVp6oFiM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "query = \"How is GitHub useful?\"\n",
        "\n",
        "# Use document_chunks as document_texts, since these are the actual text chunks\n",
        "relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "\n",
        "# Print the relevant text chunks and their distances\n",
        "for i, chunk in enumerate(relevant_chunks):\n",
        "    print(f\"Chunk {i + 1}: {chunk} (Distance: {distances[i]})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZCPzcJ5UL7S",
        "outputId": "280dc959-40c1-41d2-b014-03863f40ec8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: proach thoroughly, explaining your decisions, challenges faced,\n",
            "and solutions.\n",
            "3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\n",
            "instructions.\n",
            "4. Submissions should include:\n",
            "○ Source code for both the notebook and the interface.\n",
            "○ A fully functional Colab noteboo (Distance: 1.2285795211791992)\n",
            "\n",
            "Chunk 2: ions, and view the bot's\n",
            "responses.\n",
            "● Example interactions demonstrating the bot's capabilities.\n",
            "Guidelines:\n",
            "● Use Docker to containerize the application for easy deployment.\n",
            "● Ensure the system can handle large documents and multiple queries without\n",
            "significant performance drops.\n",
            "● Share the code,  (Distance: 1.3773610591888428)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AFTER UPDATE**"
      ],
      "metadata": {
        "id": "iMFQzTVLWexg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changes Made:**\n",
        "* top k = 5 instead of 2 for better context\n"
      ],
      "metadata": {
        "id": "yzxARmw-nfYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_texts, top_k=5):\n",
        "    # Step 1: Encode the query into an embedding\n",
        "    query_embedding = model.encode([query])[0].astype(np.float32)  # Convert to float32 for FAISS compatibility\n",
        "\n",
        "    # Step 2: Reshape the query embedding for FAISS (it should be 2D)\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    # Step 3: Perform the search using the FAISS index\n",
        "    distances, indices = index.search(query_embedding, top_k)  # Search for top_k nearest neighbors\n",
        "\n",
        "    # Step 4: Retrieve the corresponding text chunks based on indices\n",
        "    relevant_chunks = [document_texts[i] for i in indices[0]]  # Use document_texts instead of embeddings\n",
        "\n",
        "    return relevant_chunks, distances[0]"
      ],
      "metadata": {
        "id": "jd8uls8BWbNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Generate answer with Cohere API"
      ],
      "metadata": {
        "id": "3uWScyeHFs2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n",
        "import cohere\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('COHERE_API_KEY') #insert your cohere api key\n",
        "cohere_client = cohere.Client(api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxnDSZoLYrYb",
        "outputId": "8c016c31-9547-4a77-a635-fd75d03c9f5e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.9.4-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.23-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.23 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.23-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.23->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.23->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading cohere-5.9.4-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading boto3-1.35.23-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.23-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, parameterized, jmespath, httpx-sse, h11, fastavro, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.35.23 botocore-1.35.23 cohere-5.9.4 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This step utilizes the Cohere API to generate a comprehensive answer based on the user's query and the retrieved relevant chunks.\n",
        "\n",
        "A Cohere client is initialized using an API key. This key grants access to Cohere's language model, which will be used for answer generation.\n",
        "\n",
        "**Generate answer**\n",
        "* The generate_answer function takes the user's query and the relevant chunks as input.\n",
        "* It constructs a prompt by combining the query and the relevant chunks, providing context for the language model.\n",
        "* The Cohere API is called with this prompt, generating a natural language response based on the provided information.\n",
        "* The generated answer is extracted from the API response and returned.\n"
      ],
      "metadata": {
        "id": "Ml6q6PpkMomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer based on relevant text\n",
        "def generate_answer(query, relevant_chunks):\n",
        "    # Directly join the relevant_chunks, as they are strings\n",
        "    context = \" \".join(relevant_chunks)\n",
        "    response = cohere_client.generate(prompt=f\"Answer the question: {query} using the document's relevant context:{context}\")\n",
        "    return response.generations[0].text"
      ],
      "metadata": {
        "id": "yj9tVc5xMmu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TESTING with the SAMPLE SET ASSIGNMENT PDF"
      ],
      "metadata": {
        "id": "vbjPE7D_oTIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Problem Statement?\"\n",
        "relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "answer = generate_answer(query, relevant_chunks)\n",
        "print(\"Your query : \", query)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETagrF_YdP0N",
        "outputId": "574582db-9659-4cc1-a2d3-8c9f85e69b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your query :  What is the Problem Statement?\n",
            "Answer:  The provided context outlines two distinct problem statements centered around developing AI engineering solutions. Part 1 of the assignment focuses on creating a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot. This entails harnessing a vector database like Pinecone DB and a generative model to construct a robust QA bot that adeptly retrieves and generates answers to user queries. \n",
            "\n",
            "Part 2, on the other hand, calls for the development of an interactive interface tailored for the QA bot conceived in Part 1. The interface must provide real-time query input and answer retrieval, enabling users to upload relevant documents to enhance the bot's performance.\n",
            "\n",
            "These two parts collectively form a comprehensive assignment, targeting both the functional core (the RAG model) and the user-centric interface (interactive design) of the QA bot construction. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QA bot function for querying a document\n",
        "To make Testing effective lets make a function which inputs the query to give answers from the **document provided**"
      ],
      "metadata": {
        "id": "t1EQE6fVqa85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QAbot(query):\n",
        "  relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "  answer = generate_answer(query, relevant_chunks)\n",
        "  print(\"Your query : \", query)\n",
        "  print(\"Answer:\", answer)\n",
        "  return"
      ],
      "metadata": {
        "id": "bNsF74jpqREV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing QA bot for the document"
      ],
      "metadata": {
        "id": "G6MK-wmHPGiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QAbot(\"What is the purpose of the document?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns4N15kLqoCJ",
        "outputId": "16939c89-8f98-4d07-85ee-5d5af2f549a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your query :  What is the purpose of the document?\n",
            "Answer:  The purpose of the document is to outline specifications for a project to develop a system that can efficiently integrate and process PDFs, store document embeddings, and provide real-time answers to user queries. The system should be able to handle multiple queries accurately and provide contextually relevant responses. The document also outlines specific steps for achieving the desired functionality and lists the expected deliverables. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QAbot(\"What is Part 1 of the assignment and which proffesion is it intended towards?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_BdSuuLrJBN",
        "outputId": "da2ea6e8-2ab5-4dad-f556-087dc55013e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your query :  What is Part 1 of the assignment and which proffesion is it intended towards?\n",
            "Answer:  Part 1 of the assignment is to develop a retrieval-augmented generation (RAG) model for a question-answering (QA) bot for a business. It is intended towards the profession of a Gen AI Engineer or Machine Learning Engineer. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QAbot(\"Which databases can be used ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPrk7DWhs6j3",
        "outputId": "db49e811-50f2-441f-e1a5-4b38411a6fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your query :  Which databases can be used ?\n",
            "Answer:  Based on the provided information, the relevant context for the question \"Which databases can be used ?\" is primarily focused on document embedding storage and retrieval efficiencies. Here's how the different options are presented:\n",
            "\n",
            "1. Using a standard, relational database (as mentioned in paragraph [3] ): This option is suitable for storing structured data but may not be the most efficient for storing and retrieving document embeddings. Standard databases are designed for structured queries and may not handle unstructured data like documents as effectively.\n",
            "\n",
            "2. Using a vector database (such as Pinecone)**: This type of database is specifically designed to work with vector data, which makes it more efficient for storing and retrieving document embeddings. Vector databases can handle the mathematical operations required for working with vector data, providing faster performance and better compatibility with embedding representations of documents.\n",
            "\n",
            "3. Using a document store: Document stores, like MongoDB, are designed to handle unstructured JSON-like documents. This may be an acceptable alternative for storing and retrieving document embeddings, though it may not be as optimized as a vector database for this specific purpose. \n",
            "\n",
            "In the context of this dataset, the recommendation is to use a vector database due to its inherent efficiency in working with document embeddings. However, it's also important to consider other factors such as scalability, cost, and specific use-case requirements when making a decision about which database to use in real-world applications. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "QAbot(\"Is there a deadline?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpTy5IUktJUx",
        "outputId": "87d84a74-7c87-4b21-80df-6da517f48c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your query :  Is there a deadline?\n",
            "Answer:  The provided text does not contain any explicit reference to a deadline for the assignment. However, it is always a good idea to confirm deadlines with your instructor, as they may have provided specific due dates for various parts of the assignment or project. To receive accurate and up-to-date information regarding deadlines, please reach out to the appropriate individual, such as your teacher or professor, who can provide you with the exact deadline or any flexibility regarding the submission date. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\"QA bot Doc\" function for inputting other document pdfs and a query"
      ],
      "metadata": {
        "id": "8iurQkxnQEue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QAbotdoc(document_path, query):\n",
        "  top_k = 5\n",
        "  document_text = extract_text_from_pdf(document_path)\n",
        "  document_chunks = [document_text[i:i + 300] for i in range(0, len(document_text), 512)]\n",
        "  document_embeddings = model.encode(document_chunks)\n",
        "  index = faiss.IndexFlatL2(embedding_dim)  # L2 distance metric\n",
        "  faiss_embeddings = np.array(document_embeddings).astype(np.float32)\n",
        "  index.add(faiss_embeddings)\n",
        "  query_embedding = model.encode([query])[0].astype(np.float32)  # Convert to float32 for FAISS compatibility\n",
        "  query_embedding = query_embedding.reshape(1, -1)  # Reshape to be 2D for FAISS\n",
        "  distances, indices = index.search(query_embedding, top_k)  # Search for 2 nearest neighbors\n",
        "  relevant_chunks = [document_chunks[i] for i in indices[0]]  # Use document_texts instead of embeddings\n",
        "  relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "\n",
        "  answer = generate_answer(query, relevant_chunks)\n",
        "  print(\"Your Query : \\n\", query)\n",
        "  print(\"Answer:\\n\", answer)\n",
        "  return\n"
      ],
      "metadata": {
        "id": "8aWFkA6gviKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Testing with other documents\n"
      ],
      "metadata": {
        "id": "Klb4DO4Byn9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Online program offer pdf"
      ],
      "metadata": {
        "id": "hGscZEbm4pqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_path_1 = \"/content/OnlineProgramOffer.pdf\"\n",
        "query = \"What is the purpose of the document?\"\n",
        "QAbotdoc(document_path_1, query)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS2WVASvyNHi",
        "outputId": "817dbe1b-1e79-4e95-f521-d966998fb462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " What is the purpose of the document?\n",
            "Answer:\n",
            "  The purpose of the document is to communicate the program fee schedule for a Post Graduate Program in Artificial Intelligence and Machine Learning, as well as important details regarding additional costs, hardware requirements, and providing contact information for admission-related queries. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Important dates and deadlines?\"\n",
        "QAbotdoc(document_path_1, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_yxhVLGy99Z",
        "outputId": "18428479-7c70-448a-8533-9e923a6de0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " Important dates and deadlines?\n",
            "Answer:\n",
            "  The answer is can be found in the provided document under the section B. Commencement Date:\n",
            "The Post Graduate Program in Artificial Intelligence and Machine Learning: Business Applications will\n",
            "commence in the month of September 2024. The commencement details and other login credentials will\n",
            "be shared with all admitted candidates soon. In case there is a  eks before the Commencement Date\n",
            "are eligible for a full refund of the amount paid in excess of the admission fee\n",
            "2. Refund or dropout requests requested more than 2 weeks before the Commencement Date\n",
            "are eligible for a 75% refund of the amount paid in excess of the admission fee\n",
            "3. Refund or dropout answers requested within 2 weeks prior to the Commencement date forfeit all paid amounts. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I dont have a laptop, what should i do?\"\n",
        "QAbotdoc(document_path_1, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpe06UDm0Fxk",
        "outputId": "14b2cba9-f978-4299-f69b-fb527d384329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " I dont have a laptop, what should i do?\n",
            "Answer:\n",
            "  You must have a laptop or desktop PC to participate in the program. You can borrow one from a friend or family member if you do not possess one. Contact the admission helpline if you need assistance or have any further questions. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Resume pdf"
      ],
      "metadata": {
        "id": "bVVar4cz03HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_path_2 = \"/content/resume.pdf\"\n"
      ],
      "metadata": {
        "id": "Y4TeWf_-Sag2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Would this candidate be relevant for AI engineer in SampleSet?\"\n",
        "QAbotdoc(document_path_2, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYteKs31sJs",
        "outputId": "e03c95a0-e0de-49c8-9cac-ff6bd93f1d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " Would this candidate be relevant for AI engineer in SampleSet?\n",
            "Answer:\n",
            "  Yes, this candidate would be relevant for an AI engineer role in SampleSet, given their experience in generative AI tools and Agile environments, as well as their technical skills and education in electronic communications. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many years of experience does the candidate have?\"\n",
        "QAbotdoc(document_path_2, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2NGjGGt2HbV",
        "outputId": "97f844b9-99ad-414b-fbd0-51b653f8e0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " How many years of experience does the candidate have?\n",
            "Answer:\n",
            "  Tejaswi Reddy has 1 year of experience in the industry. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the candidates educational qualifications?\"\n",
        "QAbotdoc(document_path_2, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdE5RSf32a0O",
        "outputId": "fec0c012-b39c-4f3b-e256-461097915a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " What are the candidates educational qualifications?\n",
            "Answer:\n",
            "  The candidate whose resume is presented here completed a Bachelor of Engineering with a focus on Electronics and Communication at the Hyderabad campus of the BITS Pilani university in India in 2024. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the skills of the candidate?\"\n",
        "QAbotdoc(document_path_2, query)"
      ],
      "metadata": {
        "id": "LJ3lqVg43Yvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487b805f-938d-4046-9330-2757a036fb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Query : \n",
            " What are the skills of the candidate?\n",
            "Answer:\n",
            "  Here is what the candidate's skills are, according to the resume:\n",
            "\n",
            "- Technical knowledge of programming languages Python and Java\n",
            "- Proficiency with Agile methodologies, specifically scaled Agile frameworks (as evidenced by courses completed)\n",
            "- Knowledge of generative AI and prompt engineering\n",
            "- Experience with designing and developing REST APIs\n",
            "- Advanced writing skills, possibly specialized in AWS (Amazon Web Services)\n",
            "\n",
            "It is worth noting that the candidate is also a USCitizen, however it is unclear whether this is relevant information regarding their skillset. \n",
            "Let me know if you would like me to clarify any of the skills listed, or rearrange the information in a more coherent manner.  I am always happy to help. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To get the relevant chunks AND answer from the query for the document\n",
        "\n",
        "Output : the exact document text it is extracting information from to generate answers for  the query\n"
      ],
      "metadata": {
        "id": "1ipRRBs2YRp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(document_path):\n",
        "  \"\"\"Processes a document to extract text, create chunks, and build a FAISS index.\"\"\"\n",
        "  document_text = extract_text_from_pdf(document_path)\n",
        "  document_chunks = [document_text[i:i + 300] for i in range(0, len(document_text), 512)]\n",
        "  document_embeddings = model.encode(document_chunks)\n",
        "  index = faiss.IndexFlatL2(embedding_dim)\n",
        "  faiss_embeddings = np.array(document_embeddings).astype(np.float32)\n",
        "  index.add(faiss_embeddings)\n",
        "  return document_chunks, document_embeddings, index"
      ],
      "metadata": {
        "id": "qRkkrt0ebf81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QAbotdoc(document_path, query):\n",
        "    \"\"\"Processes a query by retrieving relevant document chunks and generating an answer.\"\"\"\n",
        "    # Process the document if not already processed\n",
        "    document_chunks, document_embeddings, index = process_document(document_path)\n",
        "\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_chunks, distances = retrieve_relevant_chunks_faiss(query, model, index, document_embeddings, document_chunks)\n",
        "\n",
        "    # Encode the query and search for relevant chunks in FAISS index\n",
        "    query_embedding = model.encode([query])[0].astype(np.float32)\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding,5)\n",
        "\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_chunks = [document_chunks[i] for i in indices[0]]\n",
        "\n",
        "    # Generate the answer\n",
        "    answer = generate_answer(query, relevant_chunks)\n",
        "    print(\"Answer extracted from document text : \", relevant_chunks)\n",
        "\n",
        "    print(\"Answer:\\n\", answer)\n",
        "    return #relevant_chunks, answer"
      ],
      "metadata": {
        "id": "R3Cj2XIdY08N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The function would now output the answer as well as the relevant chunks of text from the document."
      ],
      "metadata": {
        "id": "7RKgYtJwtW1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docpath = '/content/OnlineProgramOffer.pdf'\n",
        "query=\"what is the program fee schedule?\"\n",
        "QAbotdoc(docpath,query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mELAm1uQafYe",
        "outputId": "48f44947-3c9b-45c7-ba6e-dfc9c627bff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer extracted from document text :  [' Fee 05-Sep-2024 USD 800\\n1st Installment 07-Oct-2024 USD 1100\\n2nd Installment 07-Nov-2024 USD 1100\\n3rd Installment 07-Dec-2024 USD 1200\\nTotal USD 4200\\nNote: You are entitled to a discount of USD 500 (Scholarship) and USD 200 (One\\nTime Full Payment). This will be adjusted against the appropriate inst', 'entioned fee schedule will lead to disqualification from the program.\\nD. Cancellation Policy:\\nPlease note that submitting the admission fee does constitute enrolling in the program and the below\\ncancellation penalties will be applied.\\n1. Full refund can only be issued within 48 hours of enrollment.\\n', '\\nhand book.\\nBy accepting this offer, you agree to our Terms of Use and Privacy Policy\\nDelivered in Collaboration with:Post Graduate Program in Artificial Intelligence and Machine\\nLearning:\\nBusiness Applications\\nAnnexure 2\\nProgram Fee Schedule\\nThe program fee for candidates pursuing Post Graduate Pro', ' for a refund.\\nCancellation must be requested in writing to the program office.\\nDelivered in Collaboration with:Post Graduate Program in Artificial Intelligence and Machine\\nLearning:\\nBusiness Applications\\nE. Program Completion Guidelines:\\nThe Post Graduate Program in Artificial Intelligence and Mach', 'Machine Learning: Business Applications. On successful completion of the program, you will receive\\na certificate from The University of Texas at Austin.\\nPlease note that your admission will be confirmed upon the receipt of payment of your admission fee of\\nUSD 800/- (Eight Hundred US Dollars) on or b']\n",
            "Answer:\n",
            "  The program fee schedule for the Post Graduate Program in Artificial Intelligence and Machine Learning: Business Applications is as follows:\n",
            "\n",
            "The total program fee is USD 4200, with the first installment due on July 7, 2024, for USD 1100. The second installment is also USD 1100 and is due on November 7, 2024. The third and final installment is USD 1200 and is due on December 7, 2024. \n",
            "\n",
            "There are additional fees and a refund policy outlined in the document in regard to the Post Graduate Program in Artificial Intelligence and Machine Learning: Business Applications. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docpath = '/content/Gen AI Engineer _ Machine Learning Engineer Assignment.pdf'\n",
        "query=\"What is Part 1 ?\"\n",
        "QAbotdoc(docpath,query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1I9LmYpdtCO",
        "outputId": "07af6a75-de41-4d39-b3cd-3d2819603325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer extracted from document text :  ['tegrate the backend from Part 1 to process the PDF, store document embeddings,\\nand provide real-time answers to user queries.\\n3. Ensure that the system can handle multiple queries efficiently and provide accurate,\\ncontextually relevant responses.\\n4. Allow users to see the retrieved document segments', 'proach thoroughly, explaining your decisions, challenges faced,\\nand solutions.\\n3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\\ninstructions.\\n4. Submissions should include:\\n○ Source code for both the notebook and the interface.\\n○ A fully functional Colab noteboo', 'Gen AI Engineer / Machine Learning Engineer Assignment\\nPart 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\\nProblem Statement:\\nDevelop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\\nbot for a business. Use a vector database like Pinecone DB and a generative model', ' Provide several example queries and the corresponding outputs.Part 2: Interactive QA Bot Interface\\nProblem Statement:\\nDevelop an interactive interface for the QA bot from Part 1, allowing users to input queries\\nand retrieve answers in real time. The interface should enable users to upload documents', \"ions, and view the bot's\\nresponses.\\n● Example interactions demonstrating the bot's capabilities.\\nGuidelines:\\n● Use Docker to containerize the application for easy deployment.\\n● Ensure the system can handle large documents and multiple queries without\\nsignificant performance drops.\\n● Share the code, \"]\n",
            "Answer:\n",
            " Part 1 is the retrieval-augmented generation (RAG) model for the question-and-answer (QA) bot. The RAG model leverages pre-existing information and content to generate insightful and accurate responses to user queries. By integrating backend processes, storing document embeddings, and providing real-time answers, the RAG model enables the QA bot to deliver valuable insights and engaging interactions. It is designed to handle various types of queries and provide contextually relevant responses while ensuring efficient handling of multiple queries and optimal performance even with large documents. The RAG model is a crucial component of the QA bot, as it allows the bot to provide intelligent and useful answers to users' questions. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 : User interface for the QA bot is in https://colab.research.google.com/drive/1NrnVZIBMROlVMVbGLanN2YxUabyjsYTd#"
      ],
      "metadata": {
        "id": "MKf_FbGWncTT"
      }
    }
  ]
}